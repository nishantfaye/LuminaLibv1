# LuminaLib Environment Configuration
DATABASE_URL=postgresql+asyncpg://lumina:lumina@localhost:5432/luminalib
STORAGE_BACKEND=local
STORAGE_PATH=./storage
# To use S3 (MinIO) instead of local storage, set:
#   STORAGE_BACKEND=s3
#   S3_ENDPOINT_URL=http://minio:9000   (inside Docker) or http://localhost:9000
#   S3_BUCKET=luminalib-books
#   S3_ACCESS_KEY=minioadmin
#   S3_SECRET_KEY=minioadmin
S3_BUCKET=luminalib-books
S3_REGION=us-east-1
S3_ENDPOINT_URL=
S3_ACCESS_KEY=
S3_SECRET_KEY=
# "llama" → uses Ollama (docker compose up spins it up automatically)
# "mock"  → deterministic in-process responses (no Ollama needed, good for offline/CI)
# "openai"→ OpenAI API (set LLM_API_KEY below)
#LLM_PROVIDER=mock
LLM_PROVIDER=llama
LLM_BASE_URL=http://localhost:11434
LLM_MODEL=llama3.2:1b
LLM_API_KEY=
REDIS_URL=redis://localhost:6379
SECRET_KEY=change-me-in-production-use-a-long-random-string
ACCESS_TOKEN_EXPIRE_MINUTES=60
